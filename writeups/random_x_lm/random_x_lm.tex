\documentclass[12pt]{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx} 
\usepackage{float}
\usepackage[caption = false]{subfig}
\usepackage{/Users/timbarry/optionFiles/mymacros}

\begin{document}
\noindent	
Tim
\begin{center}
\textbf{Limiting variance of a regression coefficient in under random-X}
\end{center}

\section{Model with intercept}

\noindent
Suppose we observe data $(x_1, y_1), \dots, (x_n, y_n)$ from the following model:
$$
\begin{cases}
y_i = \beta_0 + \beta x_i + \epsilon_i \\
x_i \sim \textrm{Bern}(\pi) \\
\epsilon_i \sim N(0,1) \\
x_i \indep \epsilon _i.
\end{cases}
$$
We estimate $\beta$ using the standard OLS estimator:
$$ \hat{\beta} = \frac{ \sum_{i=1}^n (x_i - \bar{x}) y_i }{ n \left( \overline{(x^2_n)} - \left( \overline{x_n} \right)^2  \right)},$$
where 
$$\overline{(x^2_n)} = \frac{1}{n}\sum_{i=1}^n x_i^2$$ and $$ \overline{x_n} = \frac{1}{n} \sum_{i=1}^n x_i.$$ Our goal is to compute $\lim_{n\to\infty} \V( \sqrt{n} \hat{\beta}).$ We have by law of total variance that
$$\V(\sqrt{n} \hat{\beta}) = \E\left[ \V(\sqrt{n}\hat{\beta} | X ) \right] + \V \left[ \E ( \sqrt{n} \hat{\beta}|X) \right],$$ where $X = [x_1, \dots, x_n]$ is the vector of $x's$. It is a well-known fact that
$$ \V(\hat{\beta} | X) = \frac{ 1 }{n \left( \overline{(x^2_n)} - \left( \overline{x_n} \right)^2  \right)}.$$ Multiplying the above equality by $n$ yields $$\V( \sqrt{n} \hat{\beta} | X ) = \frac{1}{\overline{(x^2_n)} - \left( \overline{x_n} \right)^2}.$$ Next, because $\hat{\beta}$ is an unbiased estimator of $\beta$, we have that
$$ \E( \sqrt{n} \hat{\beta} | X ) = \sqrt{n} \E(\hat{\beta}|X)  = \sqrt{n} \beta.$$ Applying law of total variance,
$$\V(\sqrt{n}\hat{\beta}) = \E \left(\frac{1}{\overline{(x^2_n)} - \left( \overline{x_n} \right)^2} \right) + n \V(\beta) = \E \left(\frac{1}{\overline{(x^2_n)} - \left( \overline{x_n} \right)^2} \right) .$$

Let the random variable $T_n$ be defined by
$$T_n = \overline{ (x_n^2)} - (\overline{x_n})^2 = \frac{1}{n}\sum_{i=1}^n x_i^2 - \left(\frac{1}{n} \sum_{i=1}^n x_i \right)^2.$$ We have by LLN that
$$\{ T_n \}_{n=1}^\infty \xrightarrow{a.s.} \V(x_i) = \pi (1-\pi).$$
Moreover, 
$$T_n \leq \frac{1}{n} \sum_{i=1}^n x_i^2 \leq \frac{1}{n} \sum_{i=1}^n 1 = 1.$$ Thus, $T_n$ is bounded for all $n$. By the continuous mapping theorem and bounded convergence theorem,
$$\lim_{n \to \infty} \V(\sqrt{n} \hat{\beta}) \xrightarrow{a.s.} \E \left[ \lim_{n \to \infty} \frac{1}{T_n} \right] = \E \left( \frac{1}{\pi(1-\pi)}\right) = \frac{1}{\pi(1-\pi)}.$$
The function $\pi \to 1/(\pi(1 - \pi))$ is strictly decreasing over $\pi \in [0,1/2]$. Moreover, this function blows up at $\pi = 0$.
% \bibliographystyle{unsrt}
% \bibliography{/Users/timbarry/Documents/optionFiles/library.bib}

\section{Model without intercept}

Consider now the no-intercept model, i.e. $\beta_0 = 0.$ Let 
$$ \hat{\beta} = \frac{ \sum_{i=1}^n x_i y_i }{ \sum_{i=1}^n x_i^2}.$$

We have that 
$$ \V(\hat{\beta}) = \E\left[\V(\hat{\beta}|X)\right] + \V \left[ \E ({\hat{\beta}}|X) \right].$$
Now,
$$ \V( \hat{\beta} | X) = \frac{\sum_{i=1}^n \V(x_i y_i | x_i) }{ \left( \sum_{i=1}^n x_i^2 \right)^2} = \frac{ \sum_{i=1}^n x_i^2 }{ \left( \sum_{i=1}^n x_i^2\right)^2 } = \frac{1}{ \sum_{i=1}^n x_i^2 } .$$
Next,
$$\E(\hat{\beta}|X) = \frac{ \sum_{i=1}^n x_i x_i \beta  }{\sum_{i=1}^n x_i^2} = \beta \left( \frac{ \sum_{i=1}^n x_i^2 }{ \sum_{i=1}^n x_i^2} \right) = \beta.$$
Therefore, applying law of total variance,
$$ \V(\hat{\beta}) = \E \left( \frac{ 1 }{ \sum_{i=1}^n x_i^2 } \right),$$ and
$$ \V (\sqrt{n} \hat{\beta}) = \E \left(\frac{1}{(1/n) \sum_{i=1}^n x_i^2}\right).$$ By WLLN,
$$ (1/n) \sum_{i=1}^n x_i^2 \xrightarrow{a.s.} \E(x_i^2) = \E(x_i) = \pi. $$ Therefore,
$$\lim_{n \to \infty} \V(\sqrt{n} \hat{\beta}) \xrightarrow{a.s.} \E \left[ \lim_{n \to \infty} \frac{1}{ \sum_{i=1}^n x_i^2 } \right] = \frac{1}{\pi}.$$ Note that this variance is different than the variance for the intercept model. (If $x_i$ were a variable such that $\E(x_i) = 0$, then the variances would coincide.)

\subsection{Errors-in-variables model without intercept}

We derive the limiting variance of the thresholding method in the no-intercept model. Consider the following model:

$$
\begin{cases}
m_i = \beta_m p_i + \ep_i \\
g_i = \beta_g p_i + \tau_i \\
p_i \sim \textrm{Bern}(\pi) \\
\ep_i, \tau_i \sim N(0,1) \\
p_i \indep \tau_i \indep \ep_i
\end{cases}
$$
Define $$\hat{p}_i = \mathbb{I}\left( g_i \geq c \right)$$ for some $c > 0$. The thresholding estimator is
$$\hat{\beta} = \frac{\sum_{i=1}^n \hat{p}_i m_i}{\sum_{i=1}^n \hat{p}_i^2}.$$
We have that
$$ \V(\hat{\beta}| p, \tau ) = \frac{ \sum_{i=1}^n \hat{p}_i^2 \V(m_i | \tau_i, p_i)}{ \left( \sum_{i=1}^n \hat{p}^2_i \right)^2 } = \frac{1}{\sum_{i=1}^n \hat{p}_i^2}.$$ Furthermore,
$$\E(\hat{\beta} | p, \tau)  = \frac{ \sum_{i=1}^n \hat{p}_i m_i}{ \sum_{i=1}^n \hat{p}^2_i} = \frac{\sum_{i=1}^n \hat{p}_i \beta_m p_i }{\sum_{i=1}^n \hat{p}_i^2} = \beta_m \left( \frac{\sum_{i=1}^n \hat{p}_i p_i}{\sum_{i=1}^n \hat{p}_i^2} \right) .$$ Thus, by law of total variance,
$$ \V(\hat{\beta}) = \E \left(\frac{ 1 }{ \sum_{i=1}^n \hat{p}_i^2}\right) + \beta_m^2 \V\left(\frac{\sum_{i=1}^n \hat{p}_i p_i}{\sum_{i=1}^n \hat{p}_i^2} \right).$$
Now,
$$\beta_m^2 \V\left(\frac{\sum_{i=1}^n \hat{p}_i p_i}{\sum_{i=1}^n \hat{p}_i^2}\right) = \frac{\beta_m^2}{ \left(\sum_{i=1}^n \hat{p}_i^2 \right)^2 } \V \left( \sum_{i=1}^n \hat{p}_i p_i \right).$$
% \begin{multline*}
% \V \left( \sum_{i=1}^n \hat{p}_i p_i \right) = \E \left[ \left( \sum_{i=1}^n \hat{p}_i p_i \right)^2\right] - \left[ \E \left( \sum_{i=1}^n \hat{p}_i p_i \right) \right]^2 \\ = \E \left[ \sum_{i=1}^n \sum_{j=1}^n \hat{p}_i p_i \hat{p}_j p_j \right] - \left[ \sum_{i=1}^n \E(\hat{p}_i p_i) \right]^2 \\ = \sum_{i=1}^n \sum_{j=1}^n \E[ \hat{p}_i p_i] \E[ \hat{p}_j p_j] - \sum_{i=1}^n \sum_{j=1}^n \E( \hat{p}_i p_i) \E(\hat{p}_j p_j).
% \end{multline*}
Next, $\hat{p}_i p_i$ is a Bernoulli random variable. We therefore can calculate its variance by calculating its mean:
$$
\E\left[ \hat{p}_i p_i \right]  = \E\left( \E\left[ \hat{p}_i p_i | p_i \right]\right) = \E \left( p_i \P( \tau_i \geq c - \beta_g p_i ) \right) = \pi\P( \tau_i \geq c - \beta_g) = \omega \pi.
$$
Thus, $$\V[\hat{p}_ip_i] = \omega \pi (1 - \omega \pi).$$ Because the $p_i$s are independent, we have
$$ \V \left( \sum_{i=1}^n \hat{p}_i p_i \right) = n \omega \pi (1 - \omega \pi).$$
We can rewrite $\V(\hat{\beta})$ as 
$$ \V(\hat{\beta}) = \E \left( \frac{1}{\sum_{i=1}^n \hat{p}_i^2} \right) + \frac{n \beta_m \omega \pi (1 - \omega \pi)}{ \left(\sum_{i=1}^n \hat{p}_i \right)^2}.$$ Multiplying the above by $n$,
$$ \V(\sqrt{n} \hat{\beta}) = \E \left( \frac{1}{ (1/n) \sum_{i=1}^n \hat{p}_i^2 } \right) + \frac{ \beta_m \omega \pi (1 - \omega \pi) }{ \left( (1/n) \sum_{i=1}^n \hat{p}_i^2 \right)^2}.$$

Taking the limit,
$$ \lim_{n \to \infty} \V(\sqrt{n} \hat{\beta}) = \frac{1}{\E[ \hat{p}_i^2]} + \frac{ \beta_m \omega \pi (1 - \omega \pi) }{ \E[\hat{p}_i^2]^2} = \frac{1}{\E[ \hat{p}_i]} + \frac{\beta_m^2 \omega \pi (1 - \omega \pi)}{\E[\hat{p}_i]^2}.$$

Next, we derive the limit of $\hat{\beta}$ in the no-intercept model. We have that 
$$\hat{\beta} = \frac{ (1/n) \sum_{i=1}^n \hat{p}_i m_i}{(1/n) \sum_{i=1}^n \hat{p}_i^2}.$$
Now,
$$\lim_{n \to \infty} (1/n) \sum_{i=1}^n \hat{p}_i^2 = \E\left[\hat{p}_i^2\right] = \E [\hat{p}_i] = \zeta (1 - \pi) + \omega \pi.$$
Next,
$$\lim_{n \to \infty} (1/n) \sum_{i=1} \hat{p}_i m_i = \E[\hat{p}_i m_i].$$ We have
$$ \E[\hat{p}_i m_i] = \E[ \hat{p}_i (\beta_m p_i + \ep_i)] = \E[\beta_m \hat{p}_i p_i + \hat{p}_i \ep_i] = \beta_m \E[ \hat{p}_i p_i] = \beta_m \omega \pi.$$ Thus,
$$ \hat{\beta} \xrightarrow{P} \frac{\beta_m \omega \pi}{ \zeta(1-\pi) + \omega \pi},$$ where $\zeta = \P\left(\tau_i \geq c \right)$ and $\omega = \P(\tau_i \geq c - \beta_g).$


\end{document}
